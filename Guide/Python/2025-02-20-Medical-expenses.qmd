---
title: "用Python处理一份卫生费用数据"
number-sections: false
toc: false
date: "2025-02-21"
author: "Simonzhou"
format: html
jupyter: python3
---

# 用Python处理卫生费用数据

本篇博客用来记录2025年1月到2月帮助学弟处理一份某二级医院2018-2023年的医疗费用，最开始用的Stata，但是越往后，越感觉到Stata的难用，以及AI对这种程序的支持程度极其有限，随改用Python来继续完成相关的分析。

## 必须配置

运行此文档需要电脑上以安装Python，并且下列包已被安装并且能被调用：

`numpy jupyter-cache pandas openpyxl`

你可以使用 `pip` 或 `conda` 进行安装： `pip install jupyter-cache`

## 初步认识数据

我们可以使用`pandas`包来查看部分原始数据，数据的基本样式如下：

```{python, cache=true}
# 安装并加载必要的包
import pandas as pd
import numpy as np

# 导入 Excel 文件
file_path = "C:/Users/asus/Desktop/test/stata/prepare.xlsx"
data = pd.read_excel(file_path, sheet_name=0, engine='openpyxl')     

# 随机抽取10个样本数据
sample_data = data.sample(n=10, random_state=42)

# 打印样本数据
print(sample_data)
```

我们可以看到，该数据的列很多，第一张表中有220列，我们需要对其进行一些筛选。

## 对数据的思考

从哪里开始是一个需要思考的问题，对于数据的认识决定了你处理问题的方向和效率。首先，理解数据的来源至关重要，这包括了解数据是如何收集的、收集过程中可能出现的偏差或错误。其次，明确数据的类型与结构也是关键步骤之一，不同类型的数据（如定量数据、定性数据）需要采用不同的分析方法。再者，对数据进行初步探索，比如通过可视化手段观察数据分布特征，或是计算一些基本统计量来了解数据的基本情况，能够帮助你更好地制定数据处理策略。

在真正开始处理数据之前，还需要考虑你的目标是什么。是为了回答一个具体的问题，还是为了探索潜在的模式？明确了目标之后，才能有针对性地选择合适的工具和技术。此外，考虑到数据质量的问题，数据清洗是不可跳过的一步，它包括去除异常值、填补缺失值等操作，这对于提高分析结果的准确性非常关键。

最后，保持对数据伦理的关注同样重要，在整个数据分析的过程中，确保遵循相关的隐私保护法规和道德标准，这样才能确保你的工作不仅有效，而且负责任。通过对数据全面而深刻的理解，你可以更加自信地从数据中提取有价值的信息，并为决策提供有力支持。

### 合并数据

这份Excel文件有6张sheet，分别是2018-2023年，首先需要检查这六张sheet中的变量是否一致：

```{python,cache=true}
import pandas as pd

# 导入 Excel 文件
file_path = "C:/Users/asus/Desktop/test/stata/prepare.xlsx"
sheet_names = ["2018", "2019", "2020", "2021", "2022", "2023"]

# 读取所有 sheet 的数据
sheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}

# 获取每个 sheet 的列名
sheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}

# 找出所有 sheet 的共同变量和不一致的变量
common_columns = set.intersection(*sheets_columns.values())
all_columns = set.union(*sheets_columns.values())
inconsistent_columns = all_columns - common_columns

# 打印结果
print("一致的变量名:")
print(common_columns)

print("\n不一致的变量名:")
print(inconsistent_columns)

# 打印每个 sheet 的变量
for sheet, columns in sheets_columns.items():
    print(f"\n{sheet} 的变量: {columns}")
```

然后剔除不一致的变量数据，同时创建一个新变量`year`，用sheet的年份对其进行填充，再按变量名对应合并6张表格的数据称为一张总表，命名为`merge-sheet.xlsx`输出到你需要存放数据的文件夹中。

变量还是太多了，那接下来对变量进行筛选，首先我们可以对所有键值为空的变量进行剔除，或者根据实际的研究需要，剔除一部分键值全部为null的变量。

这里我选择对键值全部为null或0的变量进行剔除。

第一次尝试的时候，打开表后进行查看，发现变量顺序很乱，没有按照原始顺序进行排列，处理办法则是在前面的变量筛选部分使用`DataFrame`的`loc`方法选择列，同时保持列的顺序。

同时为了节省时间，因为在Quarto中运行Python代码很慢，暂时还不知道原因，待以后调试一下。所以最后用一个程序解决上述这些问题，节省时间。

```{python, cache=true}
import pandas as pd

# 导入 Excel 文件
file_path = "C:/Users/asus/Desktop/test/stata/prepare.xlsx"
output_path = "C:/Users/asus/Desktop/test/stata/data/merge-data.xlsx"
final_output_path = "C:/Users/asus/Desktop/test/stata/data/cleaned-merge-data.xlsx"
sheet_names = ["2018", "2019", "2020", "2021", "2022", "2023"]

# 读取所有 sheet 的数据
sheets_data = {sheet: pd.read_excel(file_path, sheet_name=sheet) for sheet in sheet_names}

# 获取每个 sheet 的列名
sheets_columns = {sheet: set(data.columns) for sheet, data in sheets_data.items()}

# 找出所有 sheet 的共同变量
common_columns = set.intersection(*sheets_columns.values())
# 保持原始顺序
common_columns = list(common_columns)  

# 剔除不一致的变量数据，并添加 year 变量
for sheet, data in sheets_data.items():
    sheets_data[sheet] = data[list(common_columns)]
    sheets_data[sheet]['year'] = sheet

# 合并所有 sheet 的数据
merged_data = pd.concat(sheets_data.values(), ignore_index=True)

# 输出合并后的数据到指定路径
merged_data.to_excel(output_path, index=False)

# 重新导入合并后的数据
merged_data = pd.read_excel(output_path)

# 剔除键值全部为 null 或 0 的变量，同时保持原始变量的顺序
non_null_columns = merged_data.dropna(axis=1, how='all').columns
non_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)].columns
valid_columns = [col for col in merged_data.columns if col in non_null_columns and col in non_zero_columns]

cleaned_data = merged_data.loc[:, valid_columns]

# 输出清理后的数据到指定路径
cleaned_data.to_excel(final_output_path, index=False)

print(f"清理后的数据已输出到 {final_output_path}")

# 展示部分数据

# 随机抽取10个样本数据
sample_data = cleaned_data.sample(n=10)

# 打印样本数据
print(sample_data)
```